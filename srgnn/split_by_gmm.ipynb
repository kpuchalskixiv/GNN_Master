{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from srgnn_pl import SRGNN_model, SRGNN_Map_Dataset, SRGNN_sampler\n",
    "from utils import fake_parser\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data & models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id='run-20240213_043223-0zuvfc9x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'yoochoose_custom', 'batchSize': 128, 'hiddenSize': 64, 'epoch': 60, 'lr': 0.001, 'lr_dc': 0.1, 'lr_dc_step': 3, 'l2': 1e-05, 'step': 3, 'patience': 6, 'nonhybrid': False, 'validation': True, 'valid_portion': 0.2, 'pretrained_embedings': True, 'unfreeze_epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./wandb/{run_id}/files/config.yaml\", \"r\") as stream:\n",
    "        config=yaml.safe_load(stream)\n",
    "\n",
    "keys=list(config.keys())\n",
    "for k in keys:\n",
    "    if k not in fake_parser().__dict__.keys():\n",
    "        del config[k]\n",
    "    else:\n",
    "        config[k]=config[k]['value']\n",
    "\n",
    "opt=fake_parser(**config)\n",
    "print(opt.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SRGNN_model.load_from_checkpoint(f\"./GNN_master/{run_id.split('-')[-1]}/checkpoints/\"+\n",
    "                                       os.listdir(f\"./GNN_master/{run_id.split('-')[-1]}/checkpoints/\")[0], opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./gmm_better_32_k-means++_64.gmm', 'rb') as gmm_file:\n",
    "    gm=pickle.load(gmm_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('../datasets/' + opt.dataset  + '/train.txt', 'rb'))\n",
    "\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
    "    n_node = 37484\n",
    "elif opt.dataset == 'yoochoose_custom':\n",
    "    n_node = 28583\n",
    "elif opt.dataset == 'yoochoose_custom_augmented':\n",
    "    n_node = 27809\n",
    "elif opt.dataset == 'yoochoose_custom_augmented_5050':\n",
    "    n_node = 27807\n",
    "else:\n",
    "    n_node = 310\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data masking start\n",
      "data masking 1\n",
      "data masking 2\n",
      "data masking 3\n",
      "done masking\n"
     ]
    }
   ],
   "source": [
    "train_dataset=SRGNN_Map_Dataset(train_data, shuffle=False)\n",
    "del train_data\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, \n",
    "                            num_workers=os.cpu_count(),  \n",
    "                            sampler=SRGNN_sampler(train_dataset, opt.batchSize, shuffle=False, drop_last=False)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get session embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23123it [02:11, 175.35it/s]                           \n"
     ]
    }
   ],
   "source": [
    "session_emb=[]\n",
    "full_sessions=[]\n",
    "for batch in tqdm(train_dataloader, total=train_dataset.length//opt.batchSize):\n",
    "    batch=[b.to('cuda') for b in batch]\n",
    "    session_emb.append(model.get_session_embeddings(batch).cpu().detach().numpy())\n",
    "session_emb=np.concatenate(session_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'../datasets/yoochoose_custom/gm_all_splits_{opt.hiddenSize}/session_embeddings.npy', session_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23123/23123 [00:26<00:00, 860.25it/s]\n"
     ]
    }
   ],
   "source": [
    "session_labels=[]\n",
    "for i in tqdm(range(ceil(session_emb.shape[0]/opt.batchSize))):\n",
    "    session_labels.append(gm.predict(session_emb[i*opt.batchSize: (i+1)*opt.batchSize]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_labels=np.concatenate(session_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       " array([ 53614, 103865, 267685,   6932,  50273, 115312, 109676, 430829,\n",
       "        110049, 115298, 123759,  15358,  99744,  75390, 144375,  40777,\n",
       "         36418,  39866, 197015,    960, 174476,  39363, 120382,   3374,\n",
       "          9112,  43901,  90290,  55582,  77412,  40844, 167788]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(session_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader\n",
    "del train_dataset\n",
    "train_data = pickle.load(open('../datasets/' + opt.dataset  + '/train.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 19.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for cluster in tqdm(np.unique(session_labels)):\n",
    "    idxs=np.arange(session_labels.shape[0])[session_labels==cluster]\n",
    "    cluster_sessions=[]\n",
    "    cluster_targets=[]\n",
    "    for i in idxs:\n",
    "        cluster_sessions.append(train_data[0][i])\n",
    "        cluster_targets.append(train_data[1][i])\n",
    "    with open(f'../datasets/{opt.dataset}/gm_all_splits_{opt.hiddenSize}/train_{cluster}.txt', 'wb') as cluster_file:\n",
    "        pickle.dump((cluster_sessions, cluster_targets), cluster_file)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
