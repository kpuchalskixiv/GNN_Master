{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from srgnn_pl import SRGNN_model, SRGNN_Map_Dataset, SRGNN_sampler\n",
    "from utils import fake_parser\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from utils import split_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data & models loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id='run-20240316_165704-5z65o3op'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'yoochoose_custom', 'batchSize': 128, 'hiddenSize': 64, 'epoch': 60, 'lr': 0.001, 'lr_dc': 0.1, 'lr_dc_step': 3, 'l2': 1e-05, 'step': 3, 'patience': 10, 'nonhybrid': False, 'validation': True, 'valid_portion': 0.125, 'pretrained_embedings': True, 'unfreeze_epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./wandb/{run_id}/files/config.yaml\", \"r\") as stream:\n",
    "        config=yaml.safe_load(stream)\n",
    "\n",
    "keys=list(config.keys())\n",
    "for k in keys:\n",
    "    if k not in fake_parser().__dict__.keys():\n",
    "        del config[k]\n",
    "    else:\n",
    "        config[k]=config[k]['value']\n",
    "\n",
    "opt=fake_parser(**config)\n",
    "print(opt.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "model=SRGNN_model.load_from_checkpoint(f\"./GNN_master/{run_id.split('-')[-1]}/checkpoints/\"+\n",
    "                                       os.listdir(f\"./GNN_master/{run_id.split('-')[-1]}/checkpoints/\")[0], opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('../datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
    "    n_node = 37484\n",
    "\n",
    "elif opt.dataset == 'yoochoose_custom':\n",
    "    n_node = 28583\n",
    "elif opt.dataset == 'yoochoose_custom_augmented':\n",
    "    n_node = 27809\n",
    "elif opt.dataset == 'yoochoose_custom_augmented_5050':\n",
    "    n_node = 27807\n",
    "else:\n",
    "    n_node = 310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data masking start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:05<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done masking\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
    "del train_data\n",
    "train_dataset=SRGNN_Map_Dataset(valid_data, shuffle=False)\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, \n",
    "                            num_workers=os.cpu_count(),  \n",
    "                            sampler=SRGNN_sampler(train_dataset, opt.batchSize, shuffle=False, drop_last=False)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369965"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get session embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2891it [00:27, 104.82it/s]                          \n"
     ]
    }
   ],
   "source": [
    "session_emb=[]\n",
    "full_sessions=[]\n",
    "\n",
    "model.to('cuda')\n",
    "for batch in tqdm(train_dataloader, total=train_dataset.length//opt.batchSize):\n",
    "    batch=[b.to('cuda') for b in batch]\n",
    "    session_emb.append(model.get_session_embeddings(batch).cpu().detach().numpy())\n",
    "session_emb=np.concatenate(session_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gm=GaussianMixture(n_components=32, n_init=1, init_params='k-means++')\n",
    "    \n",
    "#session_labels=gm.fit_predict(session_emb)\n",
    "\n",
    "#with open(f'./GMMs/gmm_train_{gm.n_components}_{gm.init_params}_{opt.hiddenSize}_{opt.dataset}.gmm', 'wb') as gmm_file:\n",
    " #   pickle.dump(gm, gmm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2891/2891 [00:03<00:00, 865.94it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f'./GMMs/gmm_val_32_k-means++_{opt.hiddenSize}_{opt.dataset}.gmm', 'rb') as gmm_file:\n",
    "    gm=pickle.load(gmm_file)\n",
    "\n",
    "session_labels=[]\n",
    "for i in tqdm(range(ceil(session_emb.shape[0]/opt.batchSize))):\n",
    "    session_labels.append(gm.predict(session_emb[i*opt.batchSize: (i+1)*opt.batchSize]))\n",
    "session_labels=np.concatenate(session_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  4,  5,  7,  8,  9, 11, 12, 13, 14, 16, 17, 18, 19, 20,\n",
       "        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),\n",
       " array([15262,  3400, 12620, 16258, 33931, 16182, 24881,  9763, 10742,\n",
       "         9150, 10000, 18495, 10551,  5619, 22822, 11449,  9393,  6975,\n",
       "        11772, 14395, 17025, 14156,  9581, 12228, 13646, 10186,  8755,\n",
       "        10728]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(session_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne=TSNE(2)\n",
    "tsne_session_embeddings=tsne.fit_transform(session_emb)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in np.unique(session_labels):\n",
    "    label_embedding=tsne_session_embeddings[session_labels==label]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), mode='markers'))\n",
    "\n",
    "fig.update_layout(title='TSNE reduced session embeddings with GM',\n",
    "                  margin=dict(l=40, r=40, t=40, b=40),\n",
    "                  width=1000, height=800)\n",
    "fig.write_html(f'./images/all_train_sessions_{opt.dataset}_{opt.hiddenSize}.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader\n",
    "del train_dataset\n",
    "train_data = valid_data#pickle.load(open('../datasets/' + opt.dataset  + '/train.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 57.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for cluster in tqdm(np.unique(session_labels)):\n",
    "    idxs=np.arange(session_labels.shape[0])[session_labels==cluster]\n",
    "    cluster_sessions=[]\n",
    "    cluster_targets=[]\n",
    "    for i in idxs:\n",
    "        cluster_sessions.append(train_data[0][i])\n",
    "        cluster_targets.append(train_data[1][i])\n",
    "    with open(f'../datasets/{opt.dataset}/gm_train_splits_{opt.hiddenSize}_{run_id.split('-')[-1]}/train_{cluster}.txt', 'wb') as cluster_file:\n",
    "        pickle.dump((cluster_sessions, cluster_targets), cluster_file)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
